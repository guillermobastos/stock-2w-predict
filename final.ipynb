{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import yfinance\n",
    "import ta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, cohen_kappa_score\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci칩n optimizada para obtener datos\n",
    "def get_stock_data(ticker, period, interval):\n",
    "    data = yfinance.download(ticker, period=period, interval=interval)\n",
    "    data.columns = data.columns.droplevel(1)\n",
    "    data.reset_index(inplace=True)\n",
    "    data.rename(columns={'Datetime': 'Date'}, inplace=True)\n",
    "    \n",
    "    # Convertir y extraer caracter칤sticas de fecha\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data['Day'] = data['Date'].dt.day\n",
    "    data['Month'] = data['Date'].dt.month\n",
    "    data['Hour'] = data['Date'].dt.hour\n",
    "    data['Year'] = data['Date'].dt.year\n",
    "    data['Dia_de_la_Semana'] = data['Date'].dt.weekday\n",
    "    data['Dia_del_A침o'] = data['Date'].dt.dayofyear\n",
    "    \n",
    "    # Asignar 1 si el cierre es mayor que la apertura (verde), 0 si es menor o igual (rojo)\n",
    "    data[\"Volumen_Color_Num\"] = (data[\"Close\"] > data[\"Open\"]).astype(int)\n",
    "\n",
    "    # Definir el tama침o de la ventana (aproximadamente 33 datos por semana)\n",
    "    ventana_semanal = 70\n",
    "\n",
    "    # Calcular el porcentaje de velas verdes en la 칰ltima semana\n",
    "    data[\"Porcentaje_Velas_Verdes\"] = data[\"Volumen_Color_Num\"].rolling(window=ventana_semanal, min_periods=1).mean()\n",
    "\n",
    "    # Asignar categor칤as\n",
    "    data[\"Tendencia_Semanal\"] = 1  # Neutral por defecto\n",
    "    data.loc[data[\"Porcentaje_Velas_Verdes\"] > 0.65, \"Tendencia_Semanal\"] = 2  # Alcista\n",
    "    data.loc[data[\"Porcentaje_Velas_Verdes\"] < 0.35, \"Tendencia_Semanal\"] = 0  # Bajista\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline para calcular indicadores t칠cnicos\n",
    "def create_technical_indicators_pipeline():\n",
    "    def calculate_indicators(df):\n",
    "        # Copia para evitar SettingWithCopyWarning\n",
    "        data = df.copy()\n",
    "        \n",
    "        # Medias m칩viles\n",
    "        data['EMA12'] = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "        data['EMA26'] = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "        data['EMA20'] = data['Close'].ewm(span=20, adjust=False).mean()\n",
    "        data['EMA50'] = data['Close'].ewm(span=50, adjust=False).mean()\n",
    "        data['SMA50'] = data['Close'].rolling(window=50).mean()\n",
    "        data['SMA20'] = data['Close'].rolling(window=20).mean()\n",
    "        \n",
    "        # Banda de Bollinger\n",
    "        data['BB_upper'] = data['Close'].rolling(window=20).mean() + 2 * data['Close'].rolling(window=20).std()\n",
    "        data['BB_lower'] = data['Close'].rolling(window=20).mean() - 2 * data['Close'].rolling(window=20).std()\n",
    "        data['BBW'] = (data['BB_upper'] - data['BB_lower']) / data['Close']\n",
    "        \n",
    "        # Volumen\n",
    "        data['Volume_MA'] = data['Volume'].rolling(window=20).mean()\n",
    "        data['Vol_Ratio_10h'] = data['Volume'] / data['Volume'].rolling(window=10).mean()\n",
    "        \n",
    "        # Retornos\n",
    "        data['Hourly_Return'] = data['Close'].pct_change() * 100\n",
    "        data['Cumulative_Return'] = data['Close'].pct_change(periods=5) * 100\n",
    "        \n",
    "        # MACD\n",
    "        data['MACD'] = data['EMA12'] - data['EMA26']\n",
    "        data['MACD_Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "        \n",
    "        # RSI\n",
    "        data['RSI'] = ta.momentum.rsi(data['Close'], window=14)\n",
    "        \n",
    "        # ATR\n",
    "        data['ATR'] = ta.volatility.average_true_range(data['High'], data['Low'], data['Close'], window=14)\n",
    "        \n",
    "        # ADX\n",
    "        data['ADX'] = ta.trend.adx(data['High'], data['Low'], data['Close'], window=14)\n",
    "        \n",
    "        # Estoc치stico\n",
    "        data['Stoch_K'] = ta.momentum.stoch(data['High'], data['Low'], data['Close'], window=14, smooth_window=3)\n",
    "        data['Stoch_D'] = ta.momentum.stoch_signal(data['High'], data['Low'], data['Close'], window=14, smooth_window=3)\n",
    "        \n",
    "        # Momentum\n",
    "        data['MOM'] = ta.momentum.roc(data['Close'], window=10)\n",
    "        \n",
    "        # OBV\n",
    "        data['OBV'] = ta.volume.on_balance_volume(data['Close'], data['Volume'])\n",
    "        \n",
    "        # CMF\n",
    "        data['CMF'] = ta.volume.chaikin_money_flow(data['High'], data['Low'], data['Close'], data['Volume'], window=20)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    return Pipeline([\n",
    "        ('technical_indicators', FunctionTransformer(calculate_indicators))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_change(percentage, percentiles):\n",
    "    if percentage <= percentiles[0]:\n",
    "        return 0  # Venta Muy Fuerte\n",
    "    elif percentage <= percentiles[1]:\n",
    "        return 1  # Venta\n",
    "    elif percentage <= percentiles[2]:\n",
    "        return 2  # Neutral\n",
    "    elif percentage <= percentiles[3]:\n",
    "        return 3  # Compra\n",
    "    else: \n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2w_indicators_pipeline():\n",
    "    def create_2w_indicators(df):\n",
    "        data = df.copy()\n",
    "\n",
    "        data['Past_Change_1d'] = (data['Close'] -data['Close'].shift(7)) / data['Close'].shift(7) * 100\n",
    "        data['Past_Change_2d'] = (data['Close'] -data['Close'].shift(14)) / data['Close'].shift(14) * 100\n",
    "        data['Past_Change_1w'] = (data['Close'] -data['Close'].shift(35)) / data['Close'].shift(35) * 100\n",
    "        data['Past_Change_2w'] = (data['Close'] -data['Close'].shift(70)) / data['Close'].shift(70) * 100\n",
    "\n",
    "        # ===================== 游늷 CAMBIO PORCENTUAL FUTURO (2 SEMANAS) =====================\n",
    "        data['Future_Change_2w'] = (data['Close'].shift(-70) - data['Close']) / data['Close'] * 100\n",
    "\n",
    "        # ===================== 游늷 FUNCI칍N PARA CLASIFICAR CAMBIOS =====================\n",
    "\n",
    "        # ===================== 游늷 CALCULAR PERCENTILES DIN츼MICOS =====================\n",
    "        percentiles_1w = np.percentile(data['Past_Change_1w'].dropna(), [15, 40,58 ,80])\n",
    "        percentiles_2w = np.percentile(data['Past_Change_2w'].dropna(), [15, 40,58 ,80])\n",
    "        percentiles_future_2w = np.percentile(data['Future_Change_2w'].dropna(), [15, 40,58 ,80])\n",
    "\n",
    "        # ===================== 游늷 APLICAR CLASIFICACI칍N =====================\n",
    "        data['Past_Class_1w'] = data['Past_Change_1w'].apply(lambda x: classify_change(x, percentiles_1w))\n",
    "        data['Past_Class_2w'] = data['Past_Change_2w'].apply(lambda x: classify_change(x, percentiles_2w))\n",
    "        data['Future_Class_2w'] = data['Future_Change_2w'].apply(lambda x: classify_change(x, percentiles_future_2w))\n",
    "        \n",
    "        # Crear una nueva columna para identificar eventos de volatilidad con el cambio pasado y el valor actual\n",
    "        data['Volatility_Spike_VH'] = (data['Past_Change_2w'] <= percentiles_2w[0]).astype(int)\n",
    "\n",
    "        # Crear una nueva columna para identificar eventos de volatilidad con el cambio pasado y el valor actual (subida moderada)\n",
    "        data['Volatility_Spike_H'] = (data['Past_Change_2w'] <= percentiles_2w[1]).astype(int)\n",
    "\n",
    "        # Crear una nueva columna para identificar eventos de volatilidad con el cambio pasado y el valor actual (subida leve)\n",
    "        data['Volatility_Spike_I'] = (data['Past_Change_2w'] <= percentiles_2w[1]).astype(int)\n",
    "        return data\n",
    "\n",
    "    return Pipeline([\n",
    "        ('indicators_past', FunctionTransformer(create_2w_indicators))\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Funci칩n para crear caracter칤sticas temporales (RSI, ADX, y Volumen)\n",
    "def create_temporal_features_pipeline(horizon):\n",
    "    def add_temporal_features(df):\n",
    "        data = df.copy()\n",
    "            \n",
    "        data['RSI_change_1w'] = data['RSI'] - data['RSI'].shift(35)\n",
    "        data['ADX_change_1w'] = data['ADX'] - data['ADX'].shift(35)\n",
    "        data['Volume_MA_change_1w'] = data['Volume_MA'] - data['Volume_MA'].shift(35)\n",
    "        data['RSI_change_2w'] = data['RSI'] - data['RSI'].shift(70)\n",
    "        data['ADX_change_2w'] = data['ADX'] - data['ADX'].shift(70)\n",
    "        \n",
    "        if horizon == '2w':  # Ajuste para el caso de 2 semanas\n",
    "            return data\n",
    "            \n",
    "        if horizon == '2m':  # Ajuste para el caso de 2 meses\n",
    "            # 游늷 Calcular el cambio de RSI en dos meses\n",
    "            data['RSI_change_2m'] = data['RSI'] - data['RSI'].shift(280)\n",
    "            data['ADX_change_2m'] = data['ADX'] - data['ADX'].shift(280)\n",
    "            data['Volume_MA_change_2m'] = data['Volume_MA'] - data['Volume_MA'].shift(280)\n",
    "            # 游늷 Calcular el cambio de RSI en un mes\n",
    "            data['Volume_MA_change_2w'] = data['Volume_MA'] - data['Volume_MA'].shift(70)\n",
    "            data['RSI_change_1m'] = data['RSI'] - data['RSI'].shift(140)\n",
    "            data['ADX_change_1m'] = data['ADX'] - data['ADX'].shift(140)\n",
    "            data['Volume_MA_change_1m'] = data['Volume_MA'] - data['Volume_MA'].shift(140)\n",
    "        return data\n",
    "\n",
    "    return Pipeline([\n",
    "        ('temporal_features', FunctionTransformer(add_temporal_features))\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_changes(df, horizon):\n",
    "    # C치lculos para cada tipo de cambio, seg칰n el per칤odo\n",
    "    df['Past_Change_1d'] = (df['Close'] - df['Close'].shift(7)) / df['Close'].shift(7) * 100\n",
    "    df['Past_Change_2d'] = (df['Close'] - df['Close'].shift(14)) / df['Close'].shift(14) * 100\n",
    "    df['Past_Change_1w'] = (df['Close'] - df['Close'].shift(35)) / df['Close'].shift(35) * 100\n",
    "    df['Past_Change_2w'] = (df['Close'] - df['Close'].shift(70)) / df['Close'].shift(70) * 100\n",
    "    # Cambio futuro para 2w\n",
    "    df['Future_Change_2w'] = (df['Close'].shift(-70) - df['Close']) / df['Close'] * 100\n",
    "    percentiles_1w = np.percentile(df['Past_Change_1w'].dropna(), [15, 40,58 ,80])\n",
    "    percentiles_2w = np.percentile(df['Past_Change_2w'].dropna(), [15, 40,58 ,80])\n",
    "    df['Past_Class_1w'] = df['Past_Change_1w'].apply(lambda x: classify_change(x, percentiles_1w))\n",
    "    df['Past_Class_2w'] = df['Past_Change_2w'].apply(lambda x: classify_change(x, percentiles_2w))\n",
    "    \n",
    "    if horizon == '2w':\n",
    "        percentiles_future_2w = np.percentile(df['Future_Change_2w'].dropna(), [15, 40,58 ,80])\n",
    "        df['Future_Class_2w'] = df['Future_Change_2w'].apply(lambda x: classify_change(x, percentiles_future_2w))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    elif horizon == '2m':\n",
    "        df['Past_Change_1m'] = (df['Close'] - df['Close'].shift(140)) / df['Close'].shift(140) * 100\n",
    "        df['Past_Change_2m'] = (df['Close'] - df['Close'].shift(280)) / df['Close'].shift(280) * 100\n",
    "\n",
    "        df['Future_Change_2m'] = (df['Close'].shift(-280) - df['Close']) / df['Close'] * 100\n",
    "        df['Future_Change_2w'] = (df['Close'].shift(-70) - df['Close']) / df['Close'] * 100\n",
    "        percentiles_1m = np.percentile(df['Past_Change_1m'].dropna(), [15, 40,58 ,80])\n",
    "        percentiles_2m = np.percentile(df['Past_Change_2m'].dropna(), [15, 40,58 ,80])\n",
    "        percentiles_future_2m = np.percentile(df['Future_Change_2m'].dropna(), [15, 40,58 ,80])\n",
    "\n",
    "        df['Past_Class_1m'] = df['Past_Change_1m'].apply(lambda x: classify_change(x, percentiles_1m))\n",
    "        df['Past_Class_2m'] = df['Past_Change_2m'].apply(lambda x: classify_change(x, percentiles_2m))\n",
    "        df['Future_Class_2m'] = df['Future_Change_2m'].apply(lambda x: classify_change(x, percentiles_future_2m))\n",
    "        \n",
    "        df['Volatility_Spike_VH_m'] = (df['Past_Change_2m'] <= percentiles_2m[0]).astype(int)\n",
    "        df['Volatility_Spike_H_m'] = (df['Past_Change_2m'] <= percentiles_2m[1]).astype(int)\n",
    "        df['Volatility_Spike_I_m'] = (df['Past_Change_2m'] <= percentiles_2m[1]).astype(int)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_previous_hours(data, column_name, percentiles, horas_a_marcar=84):\n",
    "    # Crear las nuevas columnas e inicializarlas con 0\n",
    "    data[f'previousd_strongsell'] = 0\n",
    "    data[f'previousd_sell'] = 0\n",
    "    data[f'previousd_strongbuy'] = 0\n",
    "    data[f'previousd_buy'] = 0\n",
    "    \n",
    "    # Iterar sobre los datos para marcar las horas previas\n",
    "    for i in range(horas_a_marcar, len(data)):  # Comenzamos desde el 칤ndice de horas_a_marcar\n",
    "        # Verificar si el cambio porcentual pasado cae por debajo del percentil 15 (fuerte ca칤da)\n",
    "        if data[column_name].iloc[i] <= percentiles[0]:\n",
    "            # Marcar las horas previas como 1 en 'previousd_strongsell'\n",
    "            data.loc[data.index[i-horas_a_marcar:i], 'previousd_strongsell'] = 1\n",
    "\n",
    "        # Verificar si el cambio porcentual pasado cae por debajo del percentil 40 (ca칤da moderada)\n",
    "        if data[column_name].iloc[i] <= percentiles[1]:\n",
    "            # Marcar las horas previas como 1 en 'previousd_sell'\n",
    "            data.loc[data.index[i-horas_a_marcar:i], 'previousd_sell'] = 1\n",
    "\n",
    "        # Verificar si el cambio porcentual pasado es mayor que el percentil 80 (fuerte subida)\n",
    "        if data[column_name].iloc[i] >= percentiles[3]:\n",
    "            # Marcar las horas previas como 1 en 'previousd_strongbuy'\n",
    "            data.loc[data.index[i-horas_a_marcar:i], 'previousd_strongbuy'] = 1\n",
    "\n",
    "        # Verificar si el cambio porcentual pasado es mayor que el percentil 58 (subida moderada)\n",
    "        if data[column_name].iloc[i] >= percentiles[2]:\n",
    "            # Marcar las horas previas como 1 en 'previousd_buy'\n",
    "            data.loc[data.index[i-horas_a_marcar:i], 'previousd_buy'] = 1\n",
    "\n",
    "    # Devolver el DataFrame con las nuevas columnas\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_features(df, horizon):\n",
    "    data = df.copy()\n",
    "    \n",
    "    correlation_matrix = data.corr()\n",
    "\n",
    "    # 游댳 Obtener las variables con correlaci칩n d칠bil con 'Future_Class_2w'\n",
    "    low_corr_features = correlation_matrix.index[\n",
    "        correlation_matrix[f'Future_Class_{horizon}'].between(-0.03, 0.03)\n",
    "    ].difference([\"Date\",\"Close\"])\n",
    "\n",
    "    # 游늷 Eliminar las columnas con baja correlaci칩n\n",
    "    data = data.drop(columns=data[low_corr_features])\n",
    "    return data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_processing_pipeline(horizon):\n",
    "    return Pipeline([\n",
    "        ('technical_indicators', create_technical_indicators_pipeline()),\n",
    "        ('indicators_past',create_2w_indicators_pipeline()),\n",
    "        ('temporal_features', create_temporal_features_pipeline(horizon)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(X_train, y_train, X_test, y_test, model_configs):\n",
    "    results = {}\n",
    "    models = {}\n",
    "    \n",
    "    for name, config in model_configs.items():\n",
    "        try:\n",
    "            # Crear pipeline con escalado y modelo\n",
    "            model_pipeline = Pipeline([\n",
    "                ('scaler', MinMaxScaler(feature_range=(0, 1))),\n",
    "                ('model', config['constructor'](**config['params']))\n",
    "            ])\n",
    "            \n",
    "            # Entrenamiento\n",
    "            model_pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Predicci칩n\n",
    "            y_pred = model_pipeline.predict(X_test)\n",
    "            \n",
    "            # M칠tricas\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "            kappa = cohen_kappa_score(y_test, y_pred)\n",
    "            \n",
    "            # Guardar resultados (solo datos serializables)\n",
    "            results[name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'f1_score': f1,\n",
    "                'kappa': kappa,\n",
    "                'predictions': y_pred.tolist(),  # Convertir a lista\n",
    "                'classification_report': classification_report(y_test, y_pred, output_dict=True)\n",
    "            }\n",
    "            \n",
    "            models[name] = model_pipeline\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error entrenando {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return results, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci칩n de modelos\n",
    "MODEL_CONFIGS = {\n",
    "    'RandomForest': {\n",
    "        'constructor': RandomForestClassifier,\n",
    "        'params': {\n",
    "            'n_estimators': 500,\n",
    "            'max_depth': 10,\n",
    "            'min_samples_split': 10,\n",
    "            'min_samples_leaf': 5,\n",
    "            'max_features': 'sqrt',\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'constructor': xgb.XGBClassifier,\n",
    "        'params': {\n",
    "            'eval_metric': 'mlogloss',\n",
    "            'n_estimators': 500,\n",
    "            'learning_rate': 0.01,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'gamma': 0.1,\n",
    "            'min_child_weight': 5,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'constructor': GradientBoostingClassifier,\n",
    "        'params': {\n",
    "            'n_estimators': 500,\n",
    "            'learning_rate': 0.01,\n",
    "            'max_depth': 6,\n",
    "            'min_samples_split': 10,\n",
    "            'min_samples_leaf': 5,\n",
    "            'subsample': 0.8,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'constructor': lgb.LGBMClassifier,\n",
    "        'params': {\n",
    "            'objective': 'multiclass',\n",
    "            'num_class': 5,\n",
    "            'n_estimators': 500,\n",
    "            'learning_rate': 0.01,\n",
    "            'max_depth': -1,\n",
    "            'num_leaves': 31,\n",
    "            'min_child_samples': 20,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'verbose': -1\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci칩n principal para ejecutar el flujo completo\n",
    "def run_analysis(ticker, period, interval, horizon='2w'):\n",
    "    # 1. Obtener datos\n",
    "    data = get_stock_data(ticker, period, interval)\n",
    "    percentiles_2w = np.percentile(data['Close'].pct_change(70).dropna() * 100, [15, 40, 58, 80])\n",
    "    \n",
    "    if horizon == '2w':\n",
    "        window = 70\n",
    "        percentiles_dict = np.percentile(data['Close'].pct_change(window).dropna() * 100, [15, 40, 58, 80])\n",
    "    else:  # '2m'\n",
    "        window = 280\n",
    "        percentiles_dict = np.percentile(data['Close'].pct_change(window).dropna() * 100, [15, 40, 58, 80])\n",
    "        \n",
    "    # 3. Procesar datos\n",
    "    processing_pipeline = create_data_processing_pipeline(horizon)\n",
    "    processed_data = processing_pipeline.fit_transform(data)\n",
    "    \n",
    "    # Llamadas a funciones\n",
    "    processed_data = calculate_changes(processed_data, horizon)\n",
    "    print(processed_data.columns)\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    processed_data = mark_previous_hours(processed_data, 'Past_Change_2w', percentiles_2w, horas_a_marcar=84)\n",
    "    processed_data = important_features(processed_data, horizon)\n",
    "    \n",
    "    # 4. Preparar datos para modelado\n",
    "    target_col = f'Future_Class_{horizon}'\n",
    "    \n",
    "    if horizon == \"2w\":\n",
    "        features = processed_data.drop(columns=['Date', 'Close', target_col, f'Future_Change_{horizon}'])\n",
    "    \n",
    "    if horizon == '2m':\n",
    "        features = processed_data.drop(columns=['Date', 'Close', target_col, f'Future_Change_{horizon}' ,'Future_Change_2w'])\n",
    "    \n",
    "    target = processed_data[target_col]\n",
    "    \n",
    "    # 5. Dividir datos (usando validaci칩n temporal)\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    for train_index, test_index in tscv.split(features):\n",
    "        X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "        y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "        \n",
    "        # 6. Entrenar y evaluar modelos\n",
    "        results, models = train_and_evaluate_models(X_train, y_train, X_test, y_test, MODEL_CONFIGS)\n",
    "        \n",
    "        # 7. Guardar modelos y resultados\n",
    "        timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "        for name, model in models.items():\n",
    "            model_filename = f\"models/{ticker}_{horizon}_{name}_{timestamp}.pkl\"\n",
    "            joblib.dump(model, model_filename)\n",
    "        \n",
    "        metrics_filename = f\"metrics/{ticker}_{horizon}_metrics_{timestamp}.json\"\n",
    "        with open(metrics_filename, 'w') as f:\n",
    "            json.dump(results, f)\n",
    "    \n",
    "    return results, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando an치lisis para 2 semanas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Close', 'High', 'Low', 'Open', 'Volume', 'Day', 'Month',\n",
      "       'Hour', 'Year', 'Dia_de_la_Semana', 'Dia_del_A침o', 'Volumen_Color_Num',\n",
      "       'Porcentaje_Velas_Verdes', 'Tendencia_Semanal', 'EMA12', 'EMA26',\n",
      "       'EMA20', 'EMA50', 'SMA50', 'SMA20', 'BB_upper', 'BB_lower', 'BBW',\n",
      "       'Volume_MA', 'Vol_Ratio_10h', 'Hourly_Return', 'Cumulative_Return',\n",
      "       'MACD', 'MACD_Signal', 'RSI', 'ATR', 'ADX', 'Stoch_K', 'Stoch_D', 'MOM',\n",
      "       'OBV', 'CMF', 'Past_Change_1d', 'Past_Change_2d', 'Past_Change_1w',\n",
      "       'Past_Change_2w', 'Future_Change_2w', 'Past_Class_1w', 'Past_Class_2w',\n",
      "       'Future_Class_2w', 'Volatility_Spike_VH', 'Volatility_Spike_H',\n",
      "       'Volatility_Spike_I', 'RSI_change_1w', 'ADX_change_1w',\n",
      "       'Volume_MA_change_1w', 'RSI_change_2w', 'ADX_change_2w'],\n",
      "      dtype='object', name='Price')\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ticker = \"AMZN\"\n",
    "    period = \"730d\"\n",
    "    interval = \"1h\"\n",
    "    \n",
    "    # Crear directorios si no existen\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "    # An치lisis para 2 semanas\n",
    "    print(\"Ejecutando an치lisis para 2 semanas...\")\n",
    "    results_2w, models_2w = run_analysis(ticker, period, interval, '2w')\n",
    "    \n",
    "    # An치lisis para 2 meses\n",
    "    print(\"\\nEjecutando an치lisis para 2 meses...\")\n",
    "    results_2m, models_2m = run_analysis(ticker, period, interval, '2m')\n",
    "    \n",
    "    # Mostrar resultados promediados\n",
    "    print(\"\\n游댳 Resultados promediados para 2 semanas:\")\n",
    "    for name, res in results_2w.items():\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Accuracy: {res['mean_accuracy']:.4f}\")\n",
    "        print(f\"  F1: {res['mean_f1']:.4f}\")\n",
    "        print(f\"  Kappa: {res['mean_kappa']:.4f}\")\n",
    "    \n",
    "    print(\"\\n游댳 Resultados promediados para 2 meses:\")\n",
    "    for name, res in results_2m.items():\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Accuracy: {res['mean_accuracy']:.4f}\")\n",
    "        print(f\"  F1: {res['mean_f1']:.4f}\")\n",
    "        print(f\"  Kappa: {res['mean_kappa']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
